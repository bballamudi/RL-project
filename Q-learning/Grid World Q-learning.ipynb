{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning on gridworld\n",
    "This example shows how the standard Q-learning algorithem can be applied to the gridworld problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set relative path to parent directory\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import and set up environment\n",
    "from environments.gridWorld import gridWorld\n",
    "env = gridWorld('../environments/gridworlds/tiny.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm\n",
    "The q learning update equation is given as\n",
    "$$ \n",
    "    Q(s, a) \\gets Q(s, a) + \\alpha[R(s, a) + \\gamma \\max_{a'}Q(s', a') - Q(s, a)]\n",
    "$$\n",
    "Where $a$ is the action in the current state, $s$ is the state, $a'$ and $s'$ is the action and state after having taken action $a$ in state $s$, $\\alpha$ is the learning rate, and $Q$ is the q-value table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "q_default = 0\n",
    "\n",
    "def Q_Learning(env, gamma, alpha, epsilon, Q = dict()):\n",
    "    # Initialize environment\n",
    "    s = env.init()\n",
    "    \n",
    "    while(env.terminal[s] == 0):\n",
    "        # Select action (epsilon greedy)\n",
    "        if np.random.rand() < epsilon:\n",
    "            a = env.actions()[np.argmax([Q.get((*s, action), q_default) for action in env.actions()])]\n",
    "        else:\n",
    "            a = np.random.choice(env.actions())\n",
    "        \n",
    "        # Perform action and observe reward\n",
    "        env.step(a)\n",
    "        r = env.reward()\n",
    "        s_next = env.state()\n",
    "        \n",
    "        # Q update\n",
    "        lst = [Q.get((*s_next, action), q_default) for action in env.actions()]\n",
    "        Q[(*s, a)] = (1-alpha)*Q.get((*s, a), q_default) + alpha*(r + (0 if (lst == []) else gamma*np.max(lst)))\n",
    "        \n",
    "        s = s_next\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training\n",
    "We train the agent by letting it interact with the environment in episodes, an episode stops when the agent reaches a terminal state. For the trainig we are using an epsilon gready strategy which means the agent will choose what it thinks is best with a probability of $\\epsilon$ , and will chose a random action with probability $1 - \\epsilon$. During the trainig phase we slowly increase epsilon. this is to promote exploration in the begining. We also have a learning rate $\\alpha$ which tells the agent how much preveouse experience should be concidered, and how much new experience should be concidered when updating the action value function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Epsilon: 0.0000, Alpha: 1.0000, Q size: 3\n",
      "Episode: 1000, Epsilon: 0.0800, Alpha: 0.9020, Q size: 40\n",
      "Episode: 2000, Epsilon: 0.1600, Alpha: 0.8040, Q size: 40\n",
      "Episode: 3000, Epsilon: 0.2400, Alpha: 0.7060, Q size: 40\n",
      "Episode: 4000, Epsilon: 0.3200, Alpha: 0.6080, Q size: 40\n",
      "Episode: 5000, Epsilon: 0.4000, Alpha: 0.5100, Q size: 40\n",
      "Episode: 6000, Epsilon: 0.4800, Alpha: 0.4120, Q size: 40\n",
      "Episode: 7000, Epsilon: 0.5600, Alpha: 0.3140, Q size: 40\n",
      "Episode: 8000, Epsilon: 0.6400, Alpha: 0.2160, Q size: 40\n",
      "Episode: 9000, Epsilon: 0.7200, Alpha: 0.1180, Q size: 40\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Q = dict()\n",
    "NUM_EPISODES = 10000\n",
    "for e in range(NUM_EPISODES):\n",
    "    epsilon = 0.80*e/NUM_EPISODES #Decaying random coice of actiondecaying \n",
    "    alpha = 1 - 0.98*e/NUM_EPISODES #Decaying learning rate\n",
    "    Q = Q_Learning(env , gamma = 1.0 , alpha = alpha, epsilon = epsilon, Q = Q)\n",
    "    if e%(NUM_EPISODES//10) == 0:\n",
    "        print(\"Episode: {}, Epsilon: {:1.4f}, Alpha: {:1.4f}, Q size: {}\".format(e, epsilon, alpha, len(Q)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib auto\n",
    "\n",
    "pos = {\"U\": (-0.15, -0.3), \"D\": (-0.15, 0.4), \"L\": (-0.45, 0.1), \"R\": (0.05, 0.1)}\n",
    "\n",
    "def show_action_value_function(Q, plot_all = True):\n",
    "    fig = env.render(show = False, show_state = False, show_reward = False)            \n",
    "    for s in env.states():\n",
    "        if plot_all:\n",
    "            for i, a in enumerate(env.actions(s)):\n",
    "                fig.axes[0].annotate(\"{0:.2f}\".format(Q[(*s, a)]), (s[1] + pos[a][0], s[0] + pos[a][1]), size = 12)\n",
    "        else: \n",
    "            lst = [Q[(*s, a)] for a in env.actions(s)]\n",
    "            if lst:\n",
    "                fig.axes[0].annotate(\"{0:.3f}\".format(max(lst)), (s[1] - 0.1, s[0] + 0.1), size = 12)\n",
    "    plt.show()\n",
    "\n",
    "show_action_value_function(Q, plot_all= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_action_value_policy(Q):\n",
    "    fig = env.render(show = False, show_state = False, show_reward = False)\n",
    "    action_map = {\"U\": \"↑\", \"D\": \"↓\", \"L\": \"←\", \"R\": \"→\"}\n",
    "    for s in env.states():\n",
    "        lst = [Q[(*s, a)] for a in env.actions(s)]\n",
    "        if lst:\n",
    "            fig.axes[0].annotate(action_map[env.actions(s)[np.argmax(lst)]], (s[1] - 0.1, s[0] + 0.1), size = 20)\n",
    "    plt.show()\n",
    "    \n",
    "show_action_value_policy(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
