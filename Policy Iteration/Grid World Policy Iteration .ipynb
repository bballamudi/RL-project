{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration on Grid World\n",
    "This notebook shows how to use policy iteration on the gridworld environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set relative path to parent directory\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import and set up environment\n",
    "from environments.gridWorld import gridWorld\n",
    "env = gridWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy evaluation algorithms\n",
    "\n",
    "For policy evaluation we wish to solve the equation\n",
    "$$ V^{\\pi}(s) = R(s, \\pi(s) + \\gamma \\sum_{s \\in S }P(s' | s, \\pi(s)) V^{\\pi}(s')$$\n",
    "\n",
    "This can be done in two ways, eiter by iteratively improving an estimate of $ V^{\\pi}(s) $ wich we call $ V^{\\pi}_{i}(s) $  by using the old estimate of the value function $ V^{\\pi}_{i-1}(s) $ until they converge ($ | V^{\\pi}_{i}(s) - V^{\\pi}_{i-1}(s)| < \\epsilon$) this gives the algorithem:\n",
    "\n",
    "Evaluate:\n",
    "$$ V^{\\pi}_{i}(s) = R(s, \\pi(s) + \\gamma \\sum_{s \\in S }P(s' | s, \\pi(s)) V^{\\pi}_{i - 1}(s')$$\n",
    "\n",
    "until: \n",
    "$$ | V^{\\pi}_{i}(s) - V^{\\pi}_{i-1}(s)| < \\epsilon$$\n",
    "\n",
    "The other method is to directly solve the linear equation by noticing that the equation can be formulated as the following linear system:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\gamma P(s_1 | s_1, \\pi(s_1) - 1 & \\gamma P(s_2 | s_1, \\pi(s_1) & \\gamma P(s_3 | s_1, \\pi(s_1) & \\dots  & \\gamma P(s_n | s_1, \\pi(s_1) \\\\\n",
    "    \\gamma P(s_1 | s_2, \\pi(s_2) & \\gamma P(s_2 | s_2, \\pi(s_2) - 1 & \\gamma P(s_3 | s_2, \\pi(s_2) & \\dots  & \\gamma P(s_n | s_2, \\pi(s_2) \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\gamma P(s_1 | s_n, \\pi(s_n) & \\gamma P(s_2 | s_n, \\pi(s_n) & \\gamma P(s_3 | s_n, \\pi(s_n) & \\dots  & \\gamma P(s_n | s_n, \\pi(s_n) - 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "    V^{\\pi}(s_1) \\\\\n",
    "    V^{\\pi}(s_2) \\\\\n",
    "    \\vdots \\\\\n",
    "    V^{\\pi}(s_n)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "    - R(s_1, \\pi(s_1) \\\\\n",
    "    - R(s_2, \\pi(s_2)\\\\\n",
    "    \\vdots \\\\\n",
    "    - R(s_n, \\pi(s_n)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "and solving for the value functions directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Evaluate the policy by iterativly improving it such that it converges\n",
    "def policy_evaluation_itrative(PI, mdp, gamma, V = dict(), epsilon = 1e-3):\n",
    "    # Initialize values to zero\n",
    "    if not V:\n",
    "        for s in env.states():\n",
    "            V.update({s: 0.0})\n",
    "    while(True):\n",
    "        V_prev = copy.deepcopy(V)\n",
    "        delta = 0\n",
    "        for s in mdp.states():\n",
    "            lst = []\n",
    "            value_sum = 0\n",
    "            for s_next in mdp.states():\n",
    "                value_sum += mdp.transition_probability(s_next, s, PI[s])*V[s_next]\n",
    "            V[s] = mdp.reward(s) + gamma*value_sum\n",
    "            if np.abs(V[s] - V_prev[s]) > delta:\n",
    "                delta = np.abs(V[s] - V_prev[s])\n",
    "        if delta < epsilon*(1 - gamma)/gamma or (gamma == 1 and delta  < epsilon):\n",
    "            return V\n",
    "        \n",
    "# Evaluate the policy by setting up and solving the linear equations\n",
    "def policy_evaluation_linalg(PI, mdp, gamma, V = dict()):\n",
    "    states = list(mdp.states())\n",
    "    size = len(states)\n",
    "\n",
    "    A = np.zeros((size, size))\n",
    "    b = np.zeros((size))\n",
    "\n",
    "    for row in range(size):\n",
    "        for col in range(size):\n",
    "            prob = mdp.transition_probability(states[col], states[row], PI[states[row]])\n",
    "            A[row, col] = gamma*prob - (1 if row == col else 0)\n",
    "            \n",
    "        b[row] = -mdp.reward(states[row])\n",
    "        \n",
    "    v = np.linalg.solve(A, b)\n",
    "    for i, s in enumerate(states):\n",
    "        V.update({s : v[i]})\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration\n",
    "The policy iteration algorithem is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def policy_iteration(mdp, gamma):\n",
    "    # Create an arbitrary policy PI and value function V\n",
    "    PI = dict()\n",
    "    V = dict()\n",
    "    for s in env.states():\n",
    "        PI.update({s : np.random.choice(env.actions())})\n",
    "        V[s] = 0\n",
    "        \n",
    "    while(True):\n",
    "        PI_prev = copy.deepcopy(PI)\n",
    "        V = policy_evaluation_linalg(PI, mdp, gamma)\n",
    "        #V = policy_evaluation_itrative(PI, mdp, gamma, V)\n",
    "        unchanged = True\n",
    "        \n",
    "        for s in mdp.states():\n",
    "            a_lst = []\n",
    "            v_lst = []\n",
    "            for a in mdp.actions(s):\n",
    "                value_sum = 0\n",
    "                for s_next in mdp.states():\n",
    "                    value_sum += mdp.transition_probability(s_next, s, a)*V[s_next]\n",
    "                v_lst.append(value_sum)\n",
    "                a_lst.append(a)\n",
    "            if len(a_lst) > 0:\n",
    "                PI.update({s: a_lst[np.argmax(v_lst)]})\n",
    "            if PI[s] != PI_prev[s]:\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return PI, V\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run policy iteration and show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PI, V= policy_iteration(env, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.812\t0.868\t0.918\t1.000\t\n",
      "0.762\tx\t0.660\t-1.000\t\n",
      "0.705\t0.655\t0.611\t0.388\t\n"
     ]
    }
   ],
   "source": [
    "for y in range(env.board_mask.shape[0]):\n",
    "    for x in range(env.board_mask.shape[1]):\n",
    "        try:\n",
    "            print('{0:.3f}'.format(V[(y, x)]), end='\\t')\n",
    "        except:\n",
    "            print('x', end='\\t')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R\tR\tR\tD\t\n",
      "U\tx\tU\tR\t\n",
      "U\tL\tL\tL\t\n"
     ]
    }
   ],
   "source": [
    "for y in range(env.board_mask.shape[0]):\n",
    "    for x in range(env.board_mask.shape[1]):\n",
    "        try:\n",
    "            print(PI[(y, x)], end = '\\t')\n",
    "        except:\n",
    "            print('x', end='\\t')\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
