{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set relative path to parent directory\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import environment\n",
    "from environments.pendulum import pendulum\n",
    "\n",
    "env = pendulum(mass=1, length=1, gravity=9.81)\n",
    "env.step_size = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor class\n",
    "The following codes sets up the neural network for the actor. The actor has two functions, the choose_action and learn functions.\n",
    "\n",
    "### The Network\n",
    "The Actor network consists of some hidden layers which learns the abstraction of the inpuut action. This then feeds into two layers which learn the standard deviation $\\sigma$, and the mean $\\mu$ of the normaldistribution (see the image below), which decides which action to take. \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "### Choosing an action\n",
    "The action to take is chosen by feeding a state into the network, and predicting the resulting standard deviation $\\sigma$ and mean $\\mu$. A random action based on the distribution is then chosen.\n",
    "\n",
    "### Learning\n",
    "The learning alorithem works by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, action_bound, hidden_layer_shape = [32], lr=0.0001):\n",
    "        self.sess = sess\n",
    "        \n",
    "        # Placeholders for action and td_error for learning, and state for learning and action selection\n",
    "        self.state = tf.placeholder(tf.float32, [None, n_features], \"state\")\n",
    "        self.action_holder = tf.placeholder(tf.float32, None, name=\"action\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, name=\"td_error\")  # TD_error\n",
    "        \n",
    "        # Actor hidden layers\n",
    "        hidden = slim.stack(inputs = self.state,\n",
    "                            layer = slim.fully_connected, \n",
    "                            stack_args = hidden_layer_shape,\n",
    "                            activation_fn = tf.nn.relu, \n",
    "                            weights_initializer = tf.random_normal_initializer(0., .1),  # weights\n",
    "                            biases_initializer = tf.constant_initializer(0.1),  # biases\n",
    "                            scope='hidden')\n",
    "        \n",
    "        # Predicted mean\n",
    "        mu = tf.layers.dense(\n",
    "            inputs=hidden,\n",
    "            units=1,\n",
    "            activation=tf.nn.tanh,\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "            name='mu'\n",
    "        )\n",
    "        \n",
    "        # Predicted standard deviation\n",
    "        sigma = tf.layers.dense(\n",
    "            inputs=hidden,\n",
    "            units=1,\n",
    "            activation=tf.nn.softplus,  # get action probabilities\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(1.),  # biases\n",
    "            name='sigma'\n",
    "        )\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        # self.e = epsilon = tf.train.exponential_decay(2., global_step, 1000, 0.9)\n",
    "        self.mu, self.sigma = mu*2, sigma + 0.1#tf.squeeze(mu*2), tf.squeeze(sigma+0.1)\n",
    "        self.normal_dist = tf.distributions.Normal(self.mu, self.sigma)\n",
    "\n",
    "        self.action = tf.clip_by_value(self.normal_dist.sample(1), action_bound[0], action_bound[1])[0]\n",
    "\n",
    "        with tf.name_scope('exp_v'):\n",
    "            log_prob = self.normal_dist.log_prob(self.action_holder)  # loss without advantage\n",
    "            self.exp_v = log_prob * self.td_error  # advantage (TD_error) guided loss\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.exp_v += 0.01*self.normal_dist.entropy()\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)    # min(v) = max(-v)\n",
    "\n",
    "    def learn(self, state, action, td_error, batch = False):\n",
    "        if batch == False:\n",
    "            state = state[np.newaxis, :]\n",
    "        feed_dict = {self.state: state, self.action_holder: action, self.td_error: td_error}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, state, batch = False):\n",
    "        if batch == False:\n",
    "            state = state[np.newaxis, :]\n",
    "        return self.sess.run(self.action, {self.state: state})  # get probabilities for all actions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic class\n",
    "The critic is responsible for estimating the value function of the policy the actor is following. It does so by using a neural network to parameterize the value function $V(s)$. To find the best parameterization of the value function the critic learns by updating the value function using the temporal difference error $r + \\gamma V(s') - V(s)$ whith the loss function as the squared temporal difference error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, hidden_layer_shape = [32, 64], lr=0.01):\n",
    "        self.sess = sess\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.state = tf.placeholder(tf.float32, [None, n_features], \"state\")\n",
    "            self.v_next = tf.placeholder(tf.float32, [None, 1], name=\"v_next\")\n",
    "            self.reward = tf.placeholder(tf.float32, None, name='reward')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            hidden = slim.stack(inputs = self.state,\n",
    "                                layer = slim.fully_connected, \n",
    "                                stack_args = hidden_layer_shape,\n",
    "                                activation_fn = tf.nn.relu, \n",
    "                                weights_initializer = tf.random_normal_initializer(0., .1),  # weights\n",
    "                                biases_initializer = tf.constant_initializer(0.1),  # biases\n",
    "                                scope='hidden'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=hidden,\n",
    "                units=1,\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='value_function'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = self.reward + GAMMA * self.v_next - self.v #tf.reduce_mean(self.reward + GAMMA * self.v_next - self.v)\n",
    "            self.loss = tf.square(tf.reduce_mean(self.td_error))    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, state, reward, state_next, batch = False):\n",
    "        if batch == False:\n",
    "            state = state[np.newaxis, :]\n",
    "            state_next = state_next[np.newaxis, :]\n",
    "        \n",
    "        # Get value of next state\n",
    "        v_next = self.sess.run(self.v, {self.state: state_next})\n",
    "        \n",
    "        # Gradient decent using td_error\n",
    "        train_dict = {self.state: state, self.v_next: v_next, self.reward: reward}\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op], train_dict)\n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experience replay buffer\n",
    "The idea behind the experience replay buffer is that by storing an agentâ€™s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. The experience is stored as a tuple $[s, a, r, s']$ where $s$ is the state we are in, $a$ is the action we take, $r$ is the reward we get, and $s'$ is the state we end up in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 1000000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array(random.sample(self.buffer,size)),[size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)\n",
    "\n",
    "# Reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Training parameters\n",
    "MAX_EPISODE = 10000\n",
    "MAX_EP_STEPS = 200\n",
    "DISPLAY_REWARD_THRESHOLD = -40\n",
    "RENDER = False\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.9\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "experience = experience_buffer()\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env = env.unwrapped\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = env.action_space.high\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# Set up actor and critic\n",
    "actor = Actor(sess, n_features=N_S, lr=LR_A, action_bound=[-A_BOUND, A_BOUND])\n",
    "critic = Critic(sess, n_features=N_S, lr=LR_C)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 1\n",
    "    ep_rs = []\n",
    "    while True:\n",
    "        if RENDER:\n",
    "            pass\n",
    "            env.render()\n",
    "        # Choose action (Actor)\n",
    "        a = actor.choose_action(s)[0]\n",
    "        \n",
    "        # Perform action and observe reward and next state\n",
    "        s_, r, done, info = env.step(a)\n",
    "        r /= 10\n",
    "        \n",
    "        # Save experience\n",
    "        experience.add(np.reshape(np.array([s,a,np.array(r),s_,np.array(done)]),[1,5]))\n",
    "        \n",
    "        if t%BATCH_SIZE == 0:\n",
    "            batch = experience.sample(BATCH_SIZE)\n",
    "            b_s = np.vstack(batch[:,0])\n",
    "            b_a = np.vstack(batch[:,1])\n",
    "            b_r = np.vstack(batch[:,2])\n",
    "            b_s_ = np.vstack(batch[:,3])\n",
    "            \n",
    "            td_error = critic.learn(b_s, b_r, b_s_, batch=True)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "            actor.learn(b_s, b_a, td_error, batch=True)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "            \n",
    "            experience.buffer = []\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "        ep_rs.append(r)\n",
    "        if t > MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(ep_rs)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "OUTPUT_GRAPH = True\n",
    "MAX_EPISODE = 10000\n",
    "MAX_EP_STEPS = 200\n",
    "DISPLAY_REWARD_THRESHOLD = -60  # renders environment if total episode reward is greater then this threshold\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.01     # learning rate for critic\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = env.action_space.high\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_S, lr=LR_A, action_bound=[-A_BOUND, A_BOUND])\n",
    "critic = Critic(sess, n_features=N_S, lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    ep_rs = []\n",
    "    while True:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        a = actor.choose_action(s)[0]\n",
    "\n",
    "        s_, r, done, info = env.step(a)\n",
    "        r /= 10\n",
    "\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "        ep_rs.append(r)\n",
    "        if t > MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(ep_rs)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
