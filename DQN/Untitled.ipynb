{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-05 18:32:52,343] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\baselines\\deepq\\build_graph.py:203: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-05 18:32:52,831] From C:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\baselines\\deepq\\build_graph.py:203: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| steps                   | 198      |\n",
      "| episodes                | 10       |\n",
      "| mean 100 episode reward | 22.1     |\n",
      "| % time spent exploring  | 98       |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2909: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| steps                   | 433      |\n",
      "| episodes                | 20       |\n",
      "| mean 100 episode reward | 22.8     |\n",
      "| % time spent exploring  | 95       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 617      |\n",
      "| episodes                | 30       |\n",
      "| mean 100 episode reward | 21.3     |\n",
      "| % time spent exploring  | 93       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 838      |\n",
      "| episodes                | 40       |\n",
      "| mean 100 episode reward | 21.5     |\n",
      "| % time spent exploring  | 91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 1.00e+03 |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | 20.5     |\n",
      "| % time spent exploring  | 90       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 1.27e+03 |\n",
      "| episodes                | 60       |\n",
      "| mean 100 episode reward | 21.5     |\n",
      "| % time spent exploring  | 87       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 1.52e+03 |\n",
      "| episodes                | 70       |\n",
      "| mean 100 episode reward | 22       |\n",
      "| % time spent exploring  | 85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 1.7e+03  |\n",
      "| episodes                | 80       |\n",
      "| mean 100 episode reward | 21.5     |\n",
      "| % time spent exploring  | 83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 2.08e+03 |\n",
      "| episodes                | 90       |\n",
      "| mean 100 episode reward | 23.3     |\n",
      "| % time spent exploring  | 79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 2.38e+03 |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | 24.1     |\n",
      "| % time spent exploring  | 76       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 2.7e+03  |\n",
      "| episodes                | 110      |\n",
      "| mean 100 episode reward | 25       |\n",
      "| % time spent exploring  | 73       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 3.08e+03 |\n",
      "| episodes                | 120      |\n",
      "| mean 100 episode reward | 26.5     |\n",
      "| % time spent exploring  | 69       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 3.52e+03 |\n",
      "| episodes                | 130      |\n",
      "| mean 100 episode reward | 29       |\n",
      "| % time spent exploring  | 65       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 3.99e+03 |\n",
      "| episodes                | 140      |\n",
      "| mean 100 episode reward | 31.6     |\n",
      "| % time spent exploring  | 60       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 4.67e+03 |\n",
      "| episodes                | 150      |\n",
      "| mean 100 episode reward | 36.7     |\n",
      "| % time spent exploring  | 54       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 5.92e+03 |\n",
      "| episodes                | 160      |\n",
      "| mean 100 episode reward | 46.6     |\n",
      "| % time spent exploring  | 41       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 7.36e+03 |\n",
      "| episodes                | 170      |\n",
      "| mean 100 episode reward | 58.5     |\n",
      "| % time spent exploring  | 27       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 9.14e+03 |\n",
      "| episodes                | 180      |\n",
      "| mean 100 episode reward | 74.4     |\n",
      "| % time spent exploring  | 10       |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: None -> 80.7\n",
      "--------------------------------------\n",
      "| steps                   | 1.09e+04 |\n",
      "| episodes                | 190      |\n",
      "| mean 100 episode reward | 88.6     |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 1.29e+04 |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | 105      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 1.47e+04 |\n",
      "| episodes                | 210      |\n",
      "| mean 100 episode reward | 120      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 1.65e+04 |\n",
      "| episodes                | 220      |\n",
      "| mean 100 episode reward | 134      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 1.8e+04  |\n",
      "| episodes                | 230      |\n",
      "| mean 100 episode reward | 145      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 1.97e+04 |\n",
      "| episodes                | 240      |\n",
      "| mean 100 episode reward | 157      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 80.7 -> 158.8\n",
      "--------------------------------------\n",
      "| steps                   | 2.11e+04 |\n",
      "| episodes                | 250      |\n",
      "| mean 100 episode reward | 165      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 2.27e+04 |\n",
      "| episodes                | 260      |\n",
      "| mean 100 episode reward | 168      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 2.41e+04 |\n",
      "| episodes                | 270      |\n",
      "| mean 100 episode reward | 167      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 2.56e+04 |\n",
      "| episodes                | 280      |\n",
      "| mean 100 episode reward | 164      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 2.71e+04 |\n",
      "| episodes                | 290      |\n",
      "| mean 100 episode reward | 162      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 2.87e+04 |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | 158      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 3e+04    |\n",
      "| episodes                | 310      |\n",
      "| mean 100 episode reward | 153      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 3.14e+04 |\n",
      "| episodes                | 320      |\n",
      "| mean 100 episode reward | 149      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 3.26e+04 |\n",
      "| episodes                | 330      |\n",
      "| mean 100 episode reward | 146      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 3.38e+04 |\n",
      "| episodes                | 340      |\n",
      "| mean 100 episode reward | 141      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 3.5e+04  |\n",
      "| episodes                | 350      |\n",
      "| mean 100 episode reward | 139      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 3.62e+04 |\n",
      "| episodes                | 360      |\n",
      "| mean 100 episode reward | 135      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| steps                   | 3.74e+04 |\n",
      "| episodes                | 370      |\n",
      "| mean 100 episode reward | 134      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 3.89e+04 |\n",
      "| episodes                | 380      |\n",
      "| mean 100 episode reward | 134      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 4.02e+04 |\n",
      "| episodes                | 390      |\n",
      "| mean 100 episode reward | 131      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 4.16e+04 |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | 129      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 4.29e+04 |\n",
      "| episodes                | 410      |\n",
      "| mean 100 episode reward | 129      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 4.41e+04 |\n",
      "| episodes                | 420      |\n",
      "| mean 100 episode reward | 128      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 4.52e+04 |\n",
      "| episodes                | 430      |\n",
      "| mean 100 episode reward | 126      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 4.64e+04 |\n",
      "| episodes                | 440      |\n",
      "| mean 100 episode reward | 126      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 4.75e+04 |\n",
      "| episodes                | 450      |\n",
      "| mean 100 episode reward | 125      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 4.87e+04 |\n",
      "| episodes                | 460      |\n",
      "| mean 100 episode reward | 125      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 5.02e+04 |\n",
      "| episodes                | 470      |\n",
      "| mean 100 episode reward | 128      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 5.15e+04 |\n",
      "| episodes                | 480      |\n",
      "| mean 100 episode reward | 126      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 5.27e+04 |\n",
      "| episodes                | 490      |\n",
      "| mean 100 episode reward | 124      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 5.38e+04 |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | 123      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 5.5e+04  |\n",
      "| episodes                | 510      |\n",
      "| mean 100 episode reward | 121      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 5.64e+04 |\n",
      "| episodes                | 520      |\n",
      "| mean 100 episode reward | 123      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 5.79e+04 |\n",
      "| episodes                | 530      |\n",
      "| mean 100 episode reward | 127      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 5.95e+04 |\n",
      "| episodes                | 540      |\n",
      "| mean 100 episode reward | 131      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 6.12e+04 |\n",
      "| episodes                | 550      |\n",
      "| mean 100 episode reward | 137      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 6.32e+04 |\n",
      "| episodes                | 560      |\n",
      "| mean 100 episode reward | 144      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 6.52e+04 |\n",
      "| episodes                | 570      |\n",
      "| mean 100 episode reward | 150      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 6.72e+04 |\n",
      "| episodes                | 580      |\n",
      "| mean 100 episode reward | 157      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 6.92e+04 |\n",
      "| episodes                | 590      |\n",
      "| mean 100 episode reward | 165      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 158.8 -> 168.4\n",
      "--------------------------------------\n",
      "| steps                   | 7.12e+04 |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | 173      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 7.32e+04 |\n",
      "| episodes                | 610      |\n",
      "| mean 100 episode reward | 182      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 7.52e+04 |\n",
      "| episodes                | 620      |\n",
      "| mean 100 episode reward | 188      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 7.72e+04 |\n",
      "| episodes                | 630      |\n",
      "| mean 100 episode reward | 193      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| steps                   | 7.92e+04 |\n",
      "| episodes                | 640      |\n",
      "| mean 100 episode reward | 197      |\n",
      "| % time spent exploring  | 2        |\n",
      "--------------------------------------\n",
      "Saving model due to mean reward increase: 168.4 -> 198.6\n",
      "Restored model with mean reward: 198.6\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\andreabm\\AppData\\Local\\Temp\\tmp53qwk989\\model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-05 18:34:46,630] Restoring parameters from C:\\Users\\andreabm\\AppData\\Local\\Temp\\tmp53qwk989\\model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to cartpole_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from baselines import deepq\n",
    "\n",
    "\n",
    "def callback(lcl, glb):\n",
    "    # stop training if reward exceeds 199\n",
    "    is_solved = lcl['t'] > 100 and sum(lcl['episode_rewards'][-101:-1]) / 100 >= 199\n",
    "    return is_solved\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    model = deepq.models.mlp([64])\n",
    "    act = deepq.learn(\n",
    "        env,\n",
    "        q_func=model,\n",
    "        lr=1e-3,\n",
    "        max_timesteps=100000,\n",
    "        buffer_size=50000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_final_eps=0.02,\n",
    "        print_freq=10,\n",
    "        callback=callback\n",
    "    )\n",
    "    print(\"Saving model to cartpole_model.pkl\")\n",
    "    act.save(\"cartpole_model.pkl\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-05 18:37:16,945] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\andreabm\\AppData\\Local\\Temp\\tmp75ohokd0\\model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-10-05 18:37:17,079] Restoring parameters from C:\\Users\\andreabm\\AppData\\Local\\Temp\\tmp75ohokd0\\model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward 200.0\n",
      "Episode reward 200.0\n",
      "Episode reward 200.0\n",
      "Episode reward 200.0\n",
      "Episode reward 200.0\n",
      "Episode reward 200.0\n",
      "Episode reward 200.0\n",
      "Episode reward 200.0\n",
      "Episode reward 200.0\n",
      "Episode reward 200.0\n",
      "Episode reward 200.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'flip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-cd2381b2a096>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-cd2381b2a096>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mepisode_rew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mepisode_rew\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36m_render\u001b[1;34m(self, mode, close)\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoletrans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 146\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    102\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mset_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'flip'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from baselines import deepq\n",
    "\n",
    "\n",
    "def main():\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    act = deepq.load(\"cartpole_model.pkl\")\n",
    "\n",
    "    while True:\n",
    "        obs, done = env.reset(), False\n",
    "        episode_rew = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            obs, rew, done, _ = env.step(act(obs[None])[0])\n",
    "            episode_rew += rew\n",
    "        print(\"Episode reward\", episode_rew)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
