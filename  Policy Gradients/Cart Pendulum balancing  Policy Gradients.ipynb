{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pendulum balancing using Policy Gradients\n",
    "This example shows how a deep q network can be created and applied to the cart pendulum problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set relative path to parent directory\n",
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath('..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import environment\n",
    "from environments.cartPendulum import cartPendulum\n",
    "\n",
    "env = cartPendulum(mass_cart=1, mass_pendulum=0.1, length_pendulum=1, gravity=9.81)\n",
    "env.step_size = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining environment functions\n",
    "We start by definin some of the functions for the environment.\n",
    "- The reward is 1 when the agent is within $0.05$ rad $\\approx 3 $ degrees oft the upright position\n",
    "- The actions available to the agent are $\\pm$ and $0$ Newtons of force on the cart\n",
    "- The terminal conditions are when the cart is $\\pm 2$m from the center or the pendulum is $>0.5$ rad from the upright position\n",
    "- The state avalable to the agent is $x, \\theta, \\dot{x}, \\dot{\\theta}, \\sin(\\theta), \\cos(\\theta)$ where $x$ is the position of the cart and $\\theta$ is the angle of the pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reward function\n",
    "env.reward = lambda : 1 if np.cos(env.state()[1]) > np.cos(0.1) else 0\n",
    "\n",
    "# Define legal actions \n",
    "env.actions = lambda : np.array([-10, -1, 0, 1, 10])\n",
    "\n",
    "# Define terminal state\n",
    "env.terminal = lambda : np.abs(env.x[0]) > 2 or np.abs(env.state()[1]) > 0.5\n",
    "\n",
    "# Define state variabels\n",
    "r = lambda theta : ((theta/np.pi - 1) % 2)*np.pi - np.pi\n",
    "env.state = lambda : np.array([env.x[0], r(env.x[1]), env.x[2], env.x[3]])#, np.sin(env.x[1]), np.cos(env.x[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the policy based agent\n",
    "we first create a discounting function which takes in the reward over time, and returns a the discounted sequence of rewards given the discounting factor $\\gamma$. The total discounted reward at time $t$ can be written as: \n",
    "$$G_t = R_t + \\gamma R_{t+1} + \\gamma^2 R_{t+2} + \\dots $$ \n",
    "Recursively this can be written as:\n",
    "$$ G_t = R_{t} + \\gamma G_{t+1} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma, normalize = False):\n",
    "    discounted_reward = np.zeros(rewards.shape)\n",
    "    for t in reversed(range(0, rewards.size)):\n",
    "        discounted_reward[t] = discounted_reward[(t + 1)%rewards.shape[0]]*gamma + rewards[t]\n",
    "    largest = np.linalg.norm(discounted_reward, np.inf)\n",
    "    return (discounted_reward/largest) if normalize and (largest > 0) else discounted_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function approximator used is a neural network, here we use a ntwork with one hidden layer, where the input is the state, and the output is the probability distribution for selecting any of the available actions. The trainig consists of simulating the environment by following some policy until we reach the terminal conditions. We then calculate the gradients of the for each time step and multiply the gradients with thediscounted reward. we then nudge the policy network in this direction in order to promote good behaveour and punish bad behaveoure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class agent():\n",
    "    def __init__(self, lr, s_size,a_size,h_size):\n",
    "        #These lines established the feed-forward part of the network. The agent takes a state and produces an action.\n",
    "        self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32)\n",
    "        hidden = slim.fully_connected(self.state_in,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(hidden,a_size,activation_fn=tf.nn.softmax,biases_initializer=None)\n",
    "        self.chosen_action = tf.argmax(self.output,1)\n",
    "\n",
    "        #The next six lines establish the training proceedure. We feed the reward and chosen action into the network\n",
    "        #to compute the loss, and use it to update the network.\n",
    "        self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        self.indexes = tf.range(0, tf.shape(self.output)[0]) * tf.shape(self.output)[1] + self.action_holder\n",
    "        self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes)\n",
    "\n",
    "        self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)*self.reward_holder)\n",
    "        \n",
    "        tvars = tf.trainable_variables()\n",
    "        self.gradient_holders = []\n",
    "        for idx,var in enumerate(tvars):\n",
    "            placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder')\n",
    "            self.gradient_holders.append(placeholder)\n",
    "        \n",
    "        self.gradients = tf.gradients(self.loss,tvars)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num episodes: 0, Mean reward: 0.0\n",
      "Num episodes: 100, Mean reward: 6.96\n",
      "Num episodes: 200, Mean reward: 4.41\n",
      "Num episodes: 300, Mean reward: 5.74\n",
      "Num episodes: 400, Mean reward: 3.87\n",
      "Num episodes: 500, Mean reward: 9.28\n",
      "Num episodes: 600, Mean reward: 6.25\n",
      "Num episodes: 700, Mean reward: 7.47\n",
      "Num episodes: 800, Mean reward: 7.32\n",
      "Num episodes: 900, Mean reward: 13.88\n",
      "Num episodes: 1000, Mean reward: 28.74\n",
      "Num episodes: 1100, Mean reward: 90.76\n",
      "Num episodes: 1200, Mean reward: 244.88\n",
      "Num episodes: 1300, Mean reward: 177.65\n",
      "Num episodes: 1400, Mean reward: 219.68\n",
      "Num episodes: 1500, Mean reward: 299.35\n",
      "Num episodes: 1600, Mean reward: 204.1\n",
      "Num episodes: 1700, Mean reward: 171.38\n",
      "Num episodes: 1800, Mean reward: 137.93\n",
      "Num episodes: 1900, Mean reward: 162.92\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # Clear the Tensorflow graph.\n",
    "\n",
    "myAgent = agent(lr = 1e-2, s_size=len(env.state()) ,a_size = len(env.actions()), h_size=12)\n",
    "saver = tf.train.Saver() # Create a tensorflow saver\n",
    "\n",
    "# Set number of episodes, max numer of steps and how often we train the nework\n",
    "total_episodes = 2000\n",
    "max_steps = 1000\n",
    "update_frequency = 5\n",
    "save_frequency = 100\n",
    "gamma = 0.99\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the tensorflow graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    #saver.restore(sess, \"./policy_net.ckpt\")\n",
    "    i = 0\n",
    "    total_reward = []\n",
    "    total_lenght = []\n",
    "        \n",
    "    gradBuffer = sess.run(tf.trainable_variables())\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "        \n",
    "    while i < total_episodes:\n",
    "        theta = (2*np.random.random() - 1)*0.4\n",
    "        state = env.init([0,theta,0,0])\n",
    "        running_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_steps):\n",
    "            #Probabilistically pick an action given our network outputs.\n",
    "            a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[state]})\n",
    "            action = np.random.choice(env.actions(),p=a_dist[0])\n",
    "            action_idx = np.argmax(env.actions() == action)\n",
    "\n",
    "            next_state = env.step(env.actions()[action_idx])\n",
    "            reward = env.reward()\n",
    "            ep_history.append([state,action_idx,reward,next_state])\n",
    "            state = next_state\n",
    "            running_reward += reward\n",
    "            if env.terminal() == True:\n",
    "                #Update the network.\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:,2] = discount_rewards(ep_history[:,2], gamma, normalize=False)\n",
    "                feed_dict={myAgent.reward_holder:ep_history[:,2],\n",
    "                        myAgent.action_holder:ep_history[:,1],myAgent.state_in:np.vstack(ep_history[:,0])}\n",
    "                grads = sess.run(myAgent.gradients, feed_dict=feed_dict)\n",
    "                for idx,grad in enumerate(grads):\n",
    "                    gradBuffer[idx] += grad\n",
    "\n",
    "                if i % update_frequency == 0 and i != 0:\n",
    "                    feed_dict= dictionary = dict(zip(myAgent.gradient_holders, gradBuffer))\n",
    "                    _ = sess.run(myAgent.update_batch, feed_dict=feed_dict)\n",
    "                    for ix,grad in enumerate(gradBuffer):\n",
    "                        gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                total_reward.append(running_reward)\n",
    "                total_lenght.append(j)\n",
    "                break\n",
    "\n",
    "        \n",
    "        # Print trainig progress and save network\n",
    "        if i % save_frequency == 0:\n",
    "            saver.save(sess, \"./policy_net.ckpt\")\n",
    "            mean_reward = np.mean(total_reward[-save_frequency:])\n",
    "            print(\"Num episodes: {}, Mean reward: {}\".format(i, mean_reward))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa3fbc171b44a5ca6563b8fa8c64aad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import widgets\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_moving_average(window_size):\n",
    "    interval = total_reward\n",
    "    window= np.ones(int(window_size))/float(window_size)\n",
    "    data = np.convolve(interval, window, 'valid')\n",
    "    plt.plot(data)\n",
    "    plt.show()\n",
    "\n",
    "slider = widgets.interact(plot_moving_average, window_size=(1, 1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "def render_policy_net(model_path, n_max_steps = 1000):\n",
    "    state = env.init([0, 0.1, 0, 0])\n",
    "    env.render()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, model_path)\n",
    "        for step in range(n_max_steps):\n",
    "            a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[state]})\n",
    "            state = env.step(env.actions()[np.argmax(a_dist)])\n",
    "            plt.pause(env.step_size)\n",
    "            if env.terminal():\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "INFO:tensorflow:Restoring parameters from ./policy_net.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andreabm\\AppData\\Local\\Continuum\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\backend_bases.py:2453: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented\n",
      "  warnings.warn(str, mplDeprecation)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib auto\n",
    "render_policy_net(\"./policy_net.ckpt\", n_max_steps=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
